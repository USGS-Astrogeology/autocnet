{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We don't technically need this but it avoids a warning when importing pysis\n",
    "import os\n",
    "os.environ['ISISROOT'] = '/usgs/cpkgs/anaconda3_linux/envs/isis3.9.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCNet Intro\n",
    "AutoCNet is a suite of functions that parallelize network generation and analyze the health of networks. You have already seen how AutoCNet can be used to analyze the health of networks, so now we will explore the parallelized network generation. \n",
    "\n",
    "The process of creating a network in AutoCNet is very similar to the general workflow of ISIS, it consists of the following steps:\n",
    "- [Load and apply configuration file for AutoCNet's associated services](#configuration)\n",
    "- [Ingest images to process and calculate corresponding overlaps](#ingest)\n",
    "- [Distribute points in overlaps](#distribute)\n",
    "- [Subpixel register relative network](#registration)\n",
    "- [Jigsaw - done in ISIS]\n",
    "\n",
    "The largest deviations from ISIS is in HOW AutoCNet goes through these steps. AutoCNet is structured to take advantage of elementwise cluster processing (these elements can be images, points, measures, etc.) and postgresql for data storage and quick relational querying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the Image Data\n",
    "We are going to process the area of the moon surrounding a Lunar Swirl named Reiner Gamma using Kaguya Terrian Camera (TC) images. Reiner Gamma is centered at (7.40, -58.80); an area for 4.9&deg; - 9.9&deg; N Planetocentric Latitude and 61.3&deg; - 56.3&deg; W Longitude was selected. The data is located in '', please use the cell below to copy the data into a directory of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = ________ # put output directory path as string here\n",
    "\n",
    "# create the output directory with a subdirectory 'updated' where the images will be copied to\n",
    "!mkdir -p $output_directory/updated/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy over the data to the 'updated' subdirectory\n",
    "!cp -p /scratch/ladoramkershner/kaguya/workshop/original/*cub $output_directory/updated/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a list of the cubes, to feed into AutoCNet. It is important that the cube list handed to AutoCNet contain **absolute** paths, as they will serve as an accessor for loading information from the cubes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls $output_directory/updated/*cub > $output_directory/cubes_updated.lis\n",
    "!head $output_directory/cubes_updated.lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configuration'></a>\n",
    "# Configuration\n",
    "AutoCNet leverages services for cluster processing and data presistance that require configuration parameters to setup properly. For the cluster processing, AutoCNet uses a module called [redis](https://redis.io/) to create queues for the cluster jobs waiting to be dispatched to the cluster, a cluster for the computations, and conda environments activated during the cluster jobs to give the cluster access to the appropriate packages. Then for the database presistence AutoCNet needs to configure the database and the spatial information the database will use when generating the geometries.\n",
    "\n",
    "In summary, AutoCNet needs configuration parameters for the following servives:\n",
    "- redis\n",
    "- cluster\n",
    "- env (short for conda environment)\n",
    "- database\n",
    "- spatial\n",
    "\n",
    "### Parse the Configuration File\n",
    "The configuration parameters are typically held in a configuration yaml file. A configuration file has been compiled for use internal to the USGS ASC facilities leveraging a shared cluster and database. Use AutoCNet's function 'parse_config' to read in the yaml file and output a dictionary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.config_parser import parse_config\n",
    "\n",
    "config_path = ________ # put path to config file as string here\n",
    "config = parse_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config is a nested dictionary, meaning it has a larger dictionary structure defining sections for the services above and then each service section is a dictionary defining the particular configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "print('configuration dictionary keys: ')\n",
    "print(np.vstack(list(config.keys())), '\\n')\n",
    "\n",
    "print('cluster configuration dictionary keys: ')\n",
    "print(np.vstack(list(config['cluster'].keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the configuration file is set up for internal use, please alter the following fields to point to user specific areas or unique strings:\n",
    "- cluster\n",
    "    - cluster_log_dir\n",
    "    - tmp_scratch_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config['cluster']['cluster_log_dir'] # show original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config['cluster']['cluster_log_dir'] = ________ # edit cluster log directory\n",
    "\n",
    "config['cluster']['cluster_log_dir'] # confirm updated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- database\n",
    "    - name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config['database']['name'] # show original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config['database']['name'] = ________ # edit database name\n",
    "\n",
    "config['database']['name'] # confirm updated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- redis\n",
    "    - basename\n",
    "    - completed_queue\n",
    "    - processing_queue\n",
    "    - working_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(config['redis']['basename'])\n",
    "print(config['redis']['completed_queue'])\n",
    "print(config['redis']['processing_queue'])\n",
    "print(config['redis']['working_queue']) # show original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# edit queue names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(config['redis']['basename'])\n",
    "print(config['redis']['completed_queue'])\n",
    "print(config['redis']['processing_queue'])\n",
    "print(config['redis']['working_queue']) # confirm updated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ingest'></a>\n",
    "# Ingest Image Data into AutoCNET\n",
    "Networks within AutoCNet are represented with an object created from a NetworkCandidateGraph (NCG) class. The NCG class is structured like an [undirected graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29), containing nodes and edges. \n",
    "\n",
    "A node in our graph is synonymous with an image. The node (image) stores path information, which serves as an accessor to the on-disk data set (this is why the absolute path was important in our cube list), and correspondences information that references the image. \n",
    "\n",
    "An edge in our graph represent the overlap relationship between two nodes (images); it contains information such as the source and destination image ids (and associated node ids), the overlap dimensions, and points/measures shared between the two nodes (images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the NetworkCandidateGraph\n",
    "The NetworkCandidateGraph (NCG) class can be instantiated to an object without any arguments. However, this NCG object requires configuration before it can be used for any meaningful work, so we have to run 'config_from_dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.graph.network import NetworkCandidateGraph\n",
    "\n",
    "ncg = NetworkCandidateGraph()\n",
    "ncg.config_from_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.clear_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Image Data and Calculate Overlaps\n",
    "At this point our ncg variable is empty, so if we try to plot the contents we will get an empty plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the images into the ncg using 'add_from_filelist', which loads the images from the passed in list and then calculates the overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filelist = f'{output_directory}/cubes_updated.lis' # this should contain absolute paths\n",
    "\n",
    "# How long will this function take?\n",
    "ncg.add_from_filelist(filelist) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we plot the ncg, we see the undirected graph, where the circles are the nodes/images and the lines are the edges/overlaps. The Kaguya TC data has a very regular overlap pattern in this area, seen by the large number of edges shared between nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the image data through the ncg, but the ncg does not presist after the notebook is shut down. To presist the network, AutoCNet leverages a database for the storage of the networks images, points, and measures. The ncg has access to this database through the ncg's 'session_scope'. Through the session_scope you can interact and execute queries on your database in pure SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with ncg.session_scope() as session:\n",
    "    img_count = session.execute(\"SELECT COUNT(*) FROM images\").fetchall()\n",
    "    print('Number of images in database: ', img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of using session.execute() can be inconvenient if working with the actual data contained within the tables. For example, to access certain information you need to know the index where that information exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with ncg.session_scope() as session:\n",
    "    img = session.execute(\"SELECT * FROM images LIMIT 1\").fetchall()\n",
    "    print('image index: ', img[0][0])\n",
    "    print('product id: ', img[0][1])\n",
    "    print('image path: ', img[0][2])\n",
    "    print('image serial number: ', img[0][3])\n",
    "    print('image ignore flag: ', img[0][4])\n",
    "#     print('image geom: ', img[0][5]) # only uncomment after looking at other output\n",
    "    print('image camera type: ', img[0][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the structure of the database changes (order of the columns or a column is added/removed) or your cannot remember the order of the columns, working with the database data in this way would be very inconvenient. So AutoCNet built models for each table of the database tables to help interface with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Images, Measures, Overlay, Points\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with ncg.session_scope() as session:\n",
    "    img = session.query(Images).first()\n",
    "    print('image index: ', img.id)\n",
    "    print('product id: ', img.name)\n",
    "    print('image path: ', img.path)\n",
    "    print('image serial number: ', img.serial)\n",
    "    print('image ignore flag: ', img.ignore)\n",
    "#     print('image geometry: ', img.geom) # only uncomment after looking at other output\n",
    "    print('image camera type: ', img.cam_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the information off of the img object is more intuitive as it is field based instead of index based. Additionally, if you uncommented the geom prints (in the two previous cells) you saw that the database geometry is stored as a binary string while the Images.geom field is a shapely Multipolygon, which has more directly accessible latitude, longitude information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 25\n",
    "with ncg.session_scope() as session:\n",
    "    imgs = session.query(Images).limit(n)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(5,10))\n",
    "    axs.set_title(f'Footprints of First {n} Images in Database')\n",
    "    for img in imgs:\n",
    "        x,y = img.geom.envelope.boundary.xy\n",
    "        axs.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distribute\"></a>\n",
    "# Place Points in Overlap\n",
    "The next step in the network generation process is to lay down points in the image overlaps. We are going to use the 'place_points_in_overlap' function to lay the points down. This function first evenly distributes points spatially into a given overlap, then it back-projects the points into the 'top' image. Once in image space, the function searches the area surrounding the measures to find interesting features to shift the measures to (this increases the chance of subpixel registration passing). The shifted measures are projected back to the ground are these updated longitudes and latitudes are used to propagate the points into all images associated with the overlap. So, this function requires:\n",
    "- An overlap (to evenly distribute points into)\n",
    "- Distribution kwargs (to decide how points are distributed into the overlap)\n",
    "- Camera type (so it knows what to expect as inputs/output for the camera model)\n",
    "- Size of the area around the measure (to search for the interesting feature)\n",
    "\n",
    "For now we will use the default size and distribution arguments, but we need to change our camera type from the default 'csm' to 'isis'. \n",
    "\n",
    "Since this function operates independently on each overlap, it is ideal for paralleization with the cluster. Before dispatching the function to the cluster we need to make the log directory from our configuration file. If a SLURM job is submitted with a log directory argument that does not exist the job will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log_dir = config['cluster']['cluster_log_dir']\n",
    "print('creating directory: ', log_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to consider the arguments for applying the function to the cluster. The application of the function to the cluster is done with the ncg.apply function. Using the apply arguments we can determine how long we want to allow the job to run (walltime), how many jobs we want running at once (arraychunk), where we want our log outputs to be directed to (log_dir), etc. These inpute mirror some of the input options you would see with an SBATCH submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all together the submission for the place_point_in_overlap jobs is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.spatial.overlap import place_points_in_overlap\n",
    "\n",
    "njobs = ncg.apply('spatial.overlap.place_points_in_overlap', \n",
    "                  on='overlaps', # start of function kwargs\n",
    "                  cam_type='isis',\n",
    "                  size=71,\n",
    "                  walltime='00:20:00', # start of apply kwargs\n",
    "                  log_dir=log_dir,\n",
    "                  arraychunk=100)\n",
    "print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are not passing in a single overlap to the apply call, instead we pass \"on = 'overlaps'\". The 'on' argument indicates which element (image, overlap, point, measure) to apply the function; ncg.apply will now apply the function to all of the overlaps in the database. You can check on the progress of your jobs using the slurm 'squeue' command with the -u (user) flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uid = ________ # put jobid int here\n",
    "!squeue -u $uid | wc -l \n",
    "!squeue -u $uid | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As jobs are put on the cluster, their corresponding log files are created. You can check how many jobs have been/ are being processed on the cluster by looking in the log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls $log_dir | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more logs are placed in the log directory, you will have to specify which array job's logs you are checking on. The naming convention of the log files generated by autocnet are 'path.to.function.function_name-jobid.arrayid_taskid.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls $log_dir | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can look at a specific array jobs by doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobid = ________ # put jobid int here\n",
    "! ls $log_dir/*$jobid* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes jobs fail to submit to the cluster, it is prudent to check the ncg queue before moving on to other cluster jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redis_orphans = ncg.queue_length\n",
    "print(\"jobs left on the queue: \", redis_orphans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reapplying a function to the cluster, you do not need to resubmit the function arguments, because those were already serialized into the queue message. However, the cluster submission arguments can be reformatted and the 'reapply' argument should be set to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# njobs = ncg.apply('spatial.overlap.place_points_in_overlap', \n",
    "#                         chunksize=redis_orphans,\n",
    "#                         arraychunk=None,\n",
    "#                         walltime='00:20:00',\n",
    "#                         log_dir=log_dir,\n",
    "#                         reapply=True)\n",
    "# print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of using of a database for data storage is that it allows for storage of and therefore quick access of geometries and how those geometries relate with other elements geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Overlay, Points, Measures\n",
    "from geoalchemy2 import functions\n",
    "from geoalchemy2.shape import to_shape\n",
    "\n",
    "\n",
    "with ncg.session_scope() as session:\n",
    "    results = (\n",
    "        session.query(\n",
    "        Overlay.id, \n",
    "        Overlay.geom.label('ogeom'), \n",
    "        Points.geom.label('pgeom')\n",
    "        )\n",
    "        .join(Points, functions.ST_Contains(Overlay.geom, Points.geom)=='True')\n",
    "        .all()\n",
    "    )\n",
    "    print('number of points: ', len(results))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10,10))\n",
    "    axs.grid()\n",
    "    \n",
    "    oid = []\n",
    "    for res in results:\n",
    "        if res.id not in oid:\n",
    "            oid.append(res.id)\n",
    "            ogeom = to_shape(res.ogeom)\n",
    "            ox, oy = ogeom.envelope.boundary.xy\n",
    "            axs.plot(ox, oy, c='k')      \n",
    "        pgeom = to_shape(res.pgeom)\n",
    "        px, py = pgeom.xy\n",
    "        axs.scatter(px, py, c='grey')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the points are not in straight lines, this is because of the shifting place_points_in_overlaps does to find interesting measure locations. The distribution of points in the overlaps looks dense in the EW, so lets try rerunning place_points_in_overlap, altering the distribution kwargs.\n",
    "\n",
    "Before rerunning place_point_in_overlap, the points and measures tables need to be cleared using ncg's 'clear_db' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Measures\n",
    "with ncg.session_scope() as session:\n",
    "    npoints = session.query(Points).count()\n",
    "    print('number of points: ', npoints)\n",
    "    \n",
    "    nmeas = session.query(Measures).count()\n",
    "    print('number of measures: ', nmeas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.clear_db(tables=['points', 'measures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Measures\n",
    "with ncg.session_scope() as session:\n",
    "    npoints = session.query(Points).count()\n",
    "    print('number of points: ', npoints)\n",
    "    \n",
    "    nmeas = session.query(Measures).count()\n",
    "    print('number of measures: ', nmeas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution argument for place_points_in_overlap requires two function inputs. Since overlaps are variable shapes and sizes, integers are not suffecient to determine proper gridding of all overlaps. Instead the distribution of points along the N to S edge of the overlap and the E to W edge of the overlap are determined based on the edge's length and a grid is built from tgese edge distributions. This way a shorter edge will recieve less points and a longer side will recieve more points.\n",
    "\n",
    "The default distribution functions are: <br />\n",
    "nspts_func=lambda x: ceil(round(x,1)\\*10) <br />\n",
    "ewpts_func=lambda x: ceil(round(x,1)\\*5) <br />\n",
    "\n",
    "** NOTICE THE NS ACTUALLY GETS USED ON THE LONGER SIDE OF THE OVERLAP, NOT NECESSARILY THE NS SIDE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ns(x):\n",
    "    from math import ceil # this import has to be in the function defintion for cluster processing\n",
    "    return ceil(round(x,1)*7)\n",
    "\n",
    "def ew(x):\n",
    "    from math import ceil # this import has to be in the function defintion for cluster processing\n",
    "    return ceil(round(x,1)*5)\n",
    "\n",
    "distribute_points_kwargs = {'nspts_func':ns, 'ewpts_func':ew, 'method':'classic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "njobs = ncg.apply('spatial.overlap.place_points_in_overlap', \n",
    "                  on='overlaps', # start of function kwargs\n",
    "                  distribute_points_kwargs=distribute_points_kwargs, # NEW LINE\n",
    "                  cam_type='isis',\n",
    "                  size=71,\n",
    "                  walltime='00:20:00', # start of apply kwargs\n",
    "                  log_dir=log_dir,\n",
    "                  arraychunk=100)\n",
    "print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the progress of your jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!squeue -u $uid | wc -l\n",
    "!squeue -u $uid | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of jobs started by looking for generated logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobid = ________ # put jobid int here\n",
    "! ls $log_dir/*$jobid* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the ncg redis queue is clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redis_orphans = ncg.queue_length\n",
    "print(\"jobs left on the queue: \", redis_orphans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reapply cluster job if there are still jobs left on the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# njobs = ncg.apply('spatial.overlap.place_points_in_overlap', \n",
    "#                         chunksize=redis_orphans,\n",
    "#                         arraychunk=None,\n",
    "#                         walltime='00:20:00',\n",
    "#                         log_dir=log_dir,\n",
    "#                         reapply=True)\n",
    "# print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the new distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Overlay, Points, Measures\n",
    "from geoalchemy2 import functions\n",
    "from geoalchemy2.shape import to_shape\n",
    "\n",
    "\n",
    "with ncg.session_scope() as session:\n",
    "    results = (\n",
    "        session.query(\n",
    "        Overlay.id, \n",
    "        Overlay.geom.label('ogeom'), \n",
    "        Points.geom.label('pgeom')\n",
    "        )\n",
    "        .join(Points, functions.ST_Contains(Overlay.geom, Points.geom)=='True')\n",
    "        .all()\n",
    "    )\n",
    "    print('number of points: ', len(results))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10,10))\n",
    "    axs.grid()\n",
    "    \n",
    "    oid = []\n",
    "    for res in results:\n",
    "        if res.id not in oid:\n",
    "            oid.append(res.id)\n",
    "            ogeom = to_shape(res.ogeom)\n",
    "            ox, oy = ogeom.envelope.boundary.xy\n",
    "            axs.plot(ox, oy, c='k')      \n",
    "        pgeom = to_shape(res.pgeom)\n",
    "        px, py = pgeom.xy\n",
    "        axs.scatter(px, py, c='grey')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"registration\"></a>\n",
    "# Subpixel Registration\n",
    "After laying down points, the next step is to subpixel register the measures, to do this we are going to use the 'subpixel_register_point' function. As the name suggests, 'subpixel_register_point' registers the measures on a single point, which makes it another great candidate for parllelization. \n",
    "\n",
    "This function chooses a reference measure, affinely transforms the other images to the reference image, and clips an image chip out of the reference image and a template chip out of the transformed images. The template chips are marched across the image chip and the maximum correlation (method defined by autocnet.matcher.naive_template.pattern_match 'metric' kwarg) value and location is saved. \n",
    "\n",
    "The solution is then evaluated to see if the maximum correlation solution is acceptable. The evaluation is done using the 'cost_func' and 'threshold' arguments. The cost_func is dependent two independent variables, the first is the distance that a point has shifted from the original, sensor identified intersection, and the second is the correlation coefficient coming out of the template matcher. The __order__ that these variables are passed in __matters__. If the cost_func solution is greater than the threshold value, the registration is successful and the point is updated. If not, the registration is unsuccessful, the point is not updated and is set to ignore.\n",
    "\n",
    "So, 'subpixel_register_point' requires the following arguments:\n",
    "- pointid\n",
    "- subpixel_template_kwargs\n",
    "- cost_func \n",
    "- threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.matcher.subpixel import subpixel_register_point\n",
    "\n",
    "subpixel_register_point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Run\n",
    "We are not going to consider the distance the measures were moved in this workflow and just look at the maximum correlation value returned by the matcher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subpixel_template_kwargs = {'image_size':(121,121), 'template_size':(61,61)} \n",
    "\n",
    "njobs = ncg.apply('matcher.subpixel.subpixel_register_point', \n",
    "                  on='points', # start of function kwargs\n",
    "                  subpixel_template_kwargs=subpixel_template_kwargs,\n",
    "                  version='simple',\n",
    "                  cost_func=lambda x,y:x*0+y,\n",
    "                  threshold=0.6, \n",
    "                  walltime=\"00:30:00\", # start of apply kwargs\n",
    "                  log_dir=log_dir,\n",
    "                  arraychunk=200,\n",
    "                  chunksize=5000) # maximum chunksize = 20,000\n",
    "\n",
    "print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the progress of your jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! squeue -u $uid | wc -l\n",
    "! squeue -u $uid | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of jobs started by looking for generated logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobid = ________ # put jobid int here\n",
    "! ls $log_dir/*$jobid* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the ncg redis queue is clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redis_orphans = ncg.queue_length\n",
    "print(\"jobs left on the queue: \", redis_orphans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reapply cluster job if there are still jobs left on the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# job_array = ncg.apply('matcher.subpixel.subpixel_register_point', \n",
    "#                       reapply=True,\n",
    "#                       chunksize=redis_orphans, \n",
    "#                       arraychunk=None,\n",
    "#                       walltime=\"00:30:00\",\n",
    "#                       log_dir=subpix1_log_dir)\n",
    "# print(job_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Point Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a point id to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pid = 265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocnet.io.db.model import Images\n",
    "from plio.io.io_gdal import GeoDataset\n",
    "from autocnet.transformation import roi\n",
    "from autocnet.utils.utils import bytescale\n",
    "\n",
    "with ncg.session_scope() as session:\n",
    "    source = session.query(Measures, Images).join(Images, Measures.imageid==Images.id).filter(Measures.pointid==pid, Measures.template_metric==1).all()\n",
    "    s_img = GeoDataset(source[0][1].path)\n",
    "    sx = source[0][0].sample\n",
    "    sy = source[0][0].line\n",
    "    \n",
    "    destination = session.query(Measures, Images).join(Images, Measures.imageid==Images.id).filter(Measures.pointid==pid, Measures.template_metric!=1).limit(1).all()\n",
    "    d_img = GeoDataset(destination[0][1].path)\n",
    "    dx = destination[0][0].sample\n",
    "    dy = destination[0][0].line\n",
    "    \n",
    "    \n",
    "image_size = (121,121)\n",
    "template_size = (61,61)\n",
    "s_roi = roi.Roi(s_img, sx, sy, size_x=image_size[0], size_y=image_size[1])\n",
    "s_image = bytescale(s_roi.clip())\n",
    "\n",
    "d_roi = roi.Roi(d_img, dx, dy, size_x=image_size[0], size_y=image_size[1])\n",
    "d_template = bytescale(d_roi.clip())\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,10));\n",
    "axs[0].imshow(s_image, cmap='Greys');\n",
    "axs[0].set_title('Reference');\n",
    "axs[1].imshow(d_template, cmap='Greys');\n",
    "axs[1].set_title('Template');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second run\n",
    "We are going to rerun the subpixel registration with larger chips to attempt to register the measures that failed first run. 'subpixel_register_point' is set up so subsequent runs can use filters which only runs the function on points with a certain property value (e.g.: points where ignore=true). It can also be rerun on all points, if this is done AutoCNet checks for a previous subpixel registration result, if the new result is better the point is updated, if the previous result is better the point is left alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subpixel_template_kwargs = {'image_size':(221,221), 'template_size':(81,81)} \n",
    "# filters = {'ignore': 'true'}\n",
    "\n",
    "njobs = ncg.apply('matcher.subpixel.subpixel_register_point', \n",
    "                  on='points', # start of function kwargs\n",
    "#                   filters=filters,\n",
    "                  subpixel_template_kwargs=subpixel_template_kwargs,\n",
    "                  version='simple',\n",
    "                  cost_func=lambda x,y:x*0+y,\n",
    "                  threshold=0.6, \n",
    "                  walltime=\"00:30:00\", # start of apply kwargs\n",
    "                  log_dir=log_dir,\n",
    "                  arraychunk=100,\n",
    "                  chunksize=5000) # maximum chunksize = 20,000\n",
    "\n",
    "\n",
    "print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the progress of your jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! squeue -u $uid | wc -l\n",
    "! squeue -u $uid | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of jobs started by looking for generated logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobid = ________ # put jobid int here\n",
    "! ls $log_dir/*$jobid* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the ncg redis queue is clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redis_orphans = ncg.queue_length\n",
    "print(\"jobs left on the queue: \", redis_orphans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reapply cluster job if there are still jobs left on the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# njobs = ncg.apply('matcher.subpixel.subpixel_register_point', \n",
    "#                   reapply = True,\n",
    "#                   walltime=\"00:30:00\",\n",
    "#                   log_dir='/scratch/ladoramkershner/mars_quads/oxia_palus/subpix2_logs/',\n",
    "#                   arraychunk=50,\n",
    "#                   chunksize=20000) # maximum chunksize = 20,000\n",
    "\n",
    "# print(njobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subpix2: Write out Network\n",
    "At this point you write out the network to begin work bundling the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnet = 'reiner_gamma_morning_ns7_ew5_t121x61_t221x81.net'\n",
    "ncg.to_isis(os.path.join(output_directory,cnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "[Loading Data from a Populated Database](#from_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"from_database\"></a>\n",
    "### Loading Data from a Populated Database\n",
    "If the database is already created, you can access the information using 'from_database'. You do not need to run this cell now, this is included in case the notebook fails or your connection is lost and you need to reload the NCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autocnet.graph.network import NetworkCandidateGraph\n",
    "from autocnet.config_parser import parse_config\n",
    "\n",
    "config_path = ________ # put path to config file as string here\n",
    "config = parse_config(config_path)\n",
    "\n",
    "ncg = NetworkCandidateGraph()\n",
    "ncg.config_from_dict(config)\n",
    "\n",
    "\n",
    "ncg.from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reloading the NCG it is useful to check the redis queue associated with the NCG and clean out any jobs that were left over during the notebook failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Cluster queue length:')\n",
    "print('before -> ', ncg.queue_length)\n",
    "print('Cleaning up cluster queue:')\n",
    "ncg.queue_flushdb()\n",
    "print('after  -> ', ncg.queue_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocnet_workshop",
   "language": "python",
   "name": "autocnet_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
